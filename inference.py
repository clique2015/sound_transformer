# -*- coding: utf-8 -*-
"""inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EZqxVVKF73FYoNTcMka3oix7hTouXguu
"""

import os
import tqdm
import numpy as np
import musdb
import random
import torch
import ffmpeg
import torchaudio
import torch.nn as nn
import sounddevice as sd
import torch.optim as optim
from IPython.display import Audio
from encodec import EncodecModel
from encodec.utils import convert_audio
from my_transformer import build_transformer

mus = musdb.DB("audio/musdb/")
for track in mus:
    print(track.name)

mus_train = musdb.DB(root="audio/musdb", subsets="train")
mus_test = musdb.DB(root="audio/musdb", subsets="test")
def sample_list():
    track = random.choice(mus.tracks)
    track.chunk_duration = 10
    track.chunk_start = random.uniform(0, track.duration - track.chunk_duration)
    x = track.audio.T
    y = track.targets['vocals'].audio.T
    Audio(x, rate=track.rate)
    yield x, y

# Load EnCodec model
model = EncodecModel.encodec_model_48khz()  # Or 48kHz
model.set_target_bandwidth(6.0)  # Optional bitrate setting
model.eval()

def get_codes(x,y):
    # Load audio file
    #waveform, sr = torchaudio.load("sample.wav")
    audio_tensor = torch.tensor(x).float()  # Transpose to (channels, samples)
    waveform = convert_audio(audio_tensor, track.rate, model.sample_rate, model.channels)
    waveform = waveform.unsqueeze(0)
    # Encode: waveform â†’ discrete tokens
    with torch.no_grad():
        encoded_frames = model.encode(waveform)  # batch size = 1

    # The tokens are here:
    #codes = [f.codes for f in encoded]  # List of quantized token tensors
    codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)  # [B, n_q, T]
    #print(f"Token shape: {codes[0].shape}")  # [num_quantizers, num_frames]
    return (codes, waveform)

def get_tokens(chunk_size, codes, waveform):
    # Transpose to [B, T, F] so we can chunk time dimension
   # codes = codes.permute(0, 2, 1)  # Now shape: [B, T, F]

    B, C, T = codes.shape
    D = codes.reshape(B * C, T)

    for i in range(D.shape[0]):  # Loop over all B*C rows
        for j in range(0, T - chunk_size + 1, chunk_size):  # Step in fixed windows
            chunk = D[i, j:j + chunk_size].unsqueeze(0)  # Shape: (1, 10)
            next_chunk = D[i, j+1:j + 1 + chunk_size].unsqueeze(0)
           # print("chunk shape:", chunk.shape)
            yield chunk, next_chunk

def generate_square_subsequent_mask(sz, device):
    """Generates an upper-triangular matrix of -inf, with zeros on the diagonal."""
    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask  # shape: [sz, sz]

vocab_size = 1024
d_model = 4           # embedding dimension
batch_size = 1
seq_len = 150
i = 0
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
transformer_ = build_transformer(vocab_size, vocab_size, seq_len, seq_len, d_model, 6, 8, 0.1, d_model*4)
optimizer = optim.Adam(transformer_.parameters(), lr=1e-4)
checkpoint_path = "models/seq_dec.pt"

if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path)
    transformer_.load_state_dict(checkpoint['model_state_dict_seq_dec'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict_seq_dec'])
    epoch = checkpoint['epoch_seq_dec']
    loss = checkpoint['loss_seq_dec']
    print(f"Checkpoint loaded: epoch {epoch}, loss {loss}")
else:
   if os.path.exists("models/transformer_seq_dec.pt"):
    checkpoint = torch.load("models/transformer_seq_dec.pt")
    transformer_.load_state_dict(checkpoint)

transformer_.to(device)
transformer_.eval()

BOS_TOKEN = 0
EOS_TOKEN = 1
chunk_size = 148
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Get data
x, y = next(sample_list())
codes, waveform = get_codes(x, y)
B = codes.size(0)  # batch size

# Loop for multiple chunks
num_iters = 5
sound_ = torch.full((B, 0), BOS_TOKEN, dtype=torch.long, device=device)

for iter_ in range(num_iters):
    if iter_ == 0:
        tokens, _ = next(get_tokens(chunk_size+1, codes, waveform))
        BOS = torch.full((tokens.size(0), 1), BOS_TOKEN, dtype=torch.long, device=device)
        tokens = torch.cat([BOS, tokens.to(device)], dim=1)  # [B, T+2]
    elif iter_ == num_iters - 1:
        tokens, _ = next(get_tokens(chunk_size+1, codes, waveform))
        EOS = torch.full((tokens.size(0), 1), EOS_TOKEN, dtype=torch.long, device=device)
        tokens = torch.cat([tokens.to(device), EOS], dim=1)  # [B, T+2]
    else:
        tokens, _ = next(get_tokens(chunk_size+2, codes, waveform))

    # Encode once
    encoded = transformer_.encode(tokens, src_mask=None)

    max_len = int(1 * tokens.size(1))
    B = tokens.size(0)

    # Initialize generated with <BOS>
    generated = torch.full((B, 1), BOS_TOKEN, dtype=torch.long, device=device)
    #generated = torch.empty((B, 0), dtype=torch.long, device=device)

    for _ in range(max_len):
        tgt_mask = generate_square_subsequent_mask(generated.size(1), generated.device)
        decoded = transformer_.decode(encoded, None, generated, tgt_mask)
        projected = transformer_.project(decoded)
        next_token_logits = projected[:, -1, :]  # [B, V]
        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # [B, 1]
        generated = torch.cat([generated, next_token], dim=1)

        if (next_token == EOS_TOKEN).all():
            break

    sound_ = torch.cat([sound_, generated], dim=1)

# Combine results from all iterations
#print("Generated sequence:", sound_)

# Ensure input waveform is correct shape: [B, C, S]
if waveform.dim() == 2:
    waveform = waveform.unsqueeze(0)

# Regenerate the encoded_frames to get scale and mean
with torch.no_grad():
    ref_encoded = model.encode(waveform)  # [EncodedFrame, ...]

codes = codes.squeeze(0)  # [4, 1515]
original_lengths = [f[0].shape[-1] for f in ref_encoded]

split_codes = torch.split(codes, original_lengths, dim=-1)

new_encoded = []
for i, (old_codes, scale) in enumerate(ref_encoded):
    code = split_codes[i].unsqueeze(0)  # Add batch dim
    new_encoded.append((code, scale))


# Double-check shapes just before decoding
#for i, (c, s) in enumerate(new_encoded):
#    print(f"Frame {i}: code shape = {c.shape}, scale shape = {s.shape}")

with torch.no_grad():
    decoded = model.decode(new_encoded)



decoded = decoded.squeeze(0)  # [C, S]
decoded = torchaudio.functional.resample(decoded, model.sample_rate, track.rate)

num_channels = track.audio.shape[1]

if decoded.shape[0] > 1 and num_channels == 1:
    decoded = decoded.mean(dim=0, keepdim=True)
if decoded.shape[0] != num_channels:
    decoded = decoded[:num_channels]

decoded = torchaudio.functional.resample(decoded, model.sample_rate, track.rate)    

# Downmix to mono if needed
if decoded.shape[0] > 1:
    decoded = decoded.mean(dim=0, keepdim=True)

# Convert to numpy and make sure shape is (samples, channels)
audio_np = decoded.cpu().numpy().T  # shape: (samples, 1)

sd.play(audio_np, samplerate=track.rate)
sd.wait()
sd.play(x.T, samplerate=track.rate)
sd.wait()  



class AudioTransformerPipeline:
    def __init__(self, model_path="models/seq_dec.pt"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.encodec = EncodecModel.encodec_model_48khz()
        self.encodec.set_target_bandwidth(6.0)
        self.encodec.to(self.device)
        self.encodec.eval()

        vocab_size=1024
        d_model = 4           # embedding dimension
        batch_size = 1
        seq_len = 150
        self.model = build_transformer(vocab_size, vocab_size, seq_len, seq_len, d_model, 6, 8, 0.1, d_model*4)

        if torch.cuda.is_available():
            checkpoint = torch.load(model_path)
        else:
            checkpoint = torch.load(model_path, map_location="cpu")

        self.model.load_state_dict(checkpoint['model_state_dict_seq_dec'])
        self.model.to(self.device)
        self.model.eval()

    def process(self, waveform, sample_rate=48000):
        # Preprocess: convert to Encodec input
        waveform = convert_audio(waveform, sample_rate, self.encodec.sample_rate, self.encodec.channels)
        waveform = waveform.unsqueeze(0).to(self.device)

        # Encode to discrete tokens
        with torch.no_grad():
            encoded = self.encodec.encode(waveform)
        lengths = [e[0].shape[-1] for e in encoded]  # [150, 150, ..., 8]       
        codes = torch.cat([e[0] for e in encoded], dim=-1)  # [1, n_q, T]
  
        # Inference (generate new tokens)
        generated = self.generate(codes)

        generated = generated.squeeze(0)  # [n_q, T]

        expected_len = sum(lengths)
        actual_len = generated.shape[-1]

        # Truncate if needed
        if actual_len > expected_len:
            print(f"Truncating generated from {actual_len} to {expected_len}")
            generated = generated[..., :expected_len]  # Keep only needed tokens

        split = torch.split(codes.squeeze(0), lengths, dim=-1)
        new_encoded = []
        for i, (old_codes, scale) in enumerate(encoded):
            generated = split[i].unsqueeze(0)  # Add batch dim
            new_encoded.append((generated, scale))
        #new_encoded = [(s.unsqueeze(0), scale) for s, (_, scale) in zip(split, encoded)]
        print("got here 2")
        with torch.no_grad():
            decoded = self.encodec.decode(new_encoded)
        print("got here 3")
        return decoded.squeeze(0).cpu()

    def generate(self, codes):
        BOS_TOKEN = 0
        EOS_TOKEN = 1
        chunk_size = 148
        codes = codes.squeeze(1)
        B, C, T = codes.shape
        D = codes.reshape(B * C, T)
        sound_ = torch.full((B, 0), BOS_TOKEN, dtype=torch.long, device=self.device)

        num_iters = 5
        for iter_ in range(num_iters):
            j = iter_ * chunk_size
            chunk = D[0, j:j+chunk_size].unsqueeze(0)
            if iter_ == 0:
                chunk = torch.cat([torch.tensor([[BOS_TOKEN]], device=self.device), chunk], dim=1)
            elif iter_ == num_iters - 1:
                chunk = torch.cat([chunk, torch.tensor([[EOS_TOKEN]], device=self.device)], dim=1)

            encoded = self.model.encode(chunk, None)

            max_len = chunk.size(1)
            generated = torch.full((B, 1), BOS_TOKEN, dtype=torch.long, device=self.device)

            for _ in range(max_len):
                tgt_mask = self.generate_mask(generated.size(1))
                decoded = self.model.decode(encoded, None, generated, tgt_mask)
                logits = self.model.project(decoded)
                next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)
                generated = torch.cat([generated, next_token], dim=1)
                if (next_token == EOS_TOKEN).all():
                    break

            sound_ = torch.cat([sound_, generated], dim=1)
        return sound_

    def generate_mask(self, size):
        mask = torch.triu(torch.ones(size, size, device=self.device) * float('-inf'), diagonal=1)
        return mask